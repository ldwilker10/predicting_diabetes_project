














# Importing standard Packages 
import pandas as pd
import numpy as np
import math 

# Importing packages for visualization 
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns


# Importing packages for statistics and modeling
import statsmodels.api as sm
import sklearn.metrics as metrics
from scipy import stats
from scipy.stats import kstest
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import RocCurveDisplay, confusion_matrix, classification_report, make_scorer
from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_curve, auc
#from sklearn.metrics import plot_confusion_matrix, plot_roc_curve , log_loss, ConfusionMatrixDisplay
from sklearn.metrics import RocCurveDisplay, log_loss, ConfusionMatrixDisplay
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import (VarianceThreshold, SelectKBest, f_regression, mutual_info_regression, 
    RFE, RFECV)
from xgboost import XGBClassifier, plot_importance
from imblearn.over_sampling import SMOTE

import warnings
warnings.filterwarnings('ignore')



# Loading Dataset
diabetes = pd.read_csv('data/diabetes_2.csv')

# Previewing general information on dataset
print(diabetes.info())

#Previewing first 5 rows 
diabetes.head()







# Dropping any duplicate rows 
diabetes = diabetes.drop_duplicates()

# Checking for missing values
diabetes.isna().sum()


#Getting value_counts for whole dataset
for col in diabetes:
        print (diabetes[col].value_counts())











# Create a new DataFrame to store the filtered data
filtered_diabetes = diabetes.copy()

# Value to drop
value_to_drop = 4.860753

# Loop through the columns and drop rows containing the value
for col in diabetes.columns:
    filtered_diabetes = filtered_diabetes[filtered_diabetes[col] != value_to_drop]

# Reset the index of the filtered DataFrame
filtered_diabetes.reset_index(drop=True, inplace=True)

# Renaming columns for fluid formatting
filtered_diabetes.rename(columns={'family_histroy': 'Family_History', 'smoking': 'Smoking', 
                                  'drinking': 'Drinking'}, inplace=True)


# Print the filtered DataFrame
print(filtered_diabetes)



# Creating new df from filtered dataset
diabetes_df = filtered_diabetes.copy()
diabetes_df.info()

# Getting number of patients with and without diabetes
print(diabetes_df['Diabetes'].value_counts())








# Creating separate dfs
# One df with variables named for comprehension 
# second df just as a baseline copy
diabetes_df_main = diabetes_df.copy()

# Renaming columns for varaible understanding 
diabetes_df.rename(columns={'FFPG': 'Final Fasting Plasma Glucose', 'FPG': 'Fasting Plasma Glucose', 
                                  'SBP': 'Systolic Blood Pressure', 'Tri': 'Triglycerides', 
                                  'DBP': 'Diastolic Blood Pressure', 'Chol': 'Cholesterol', 
                                  'CCR': 'Creatine Clearance'}, inplace=True)


# Checking correlations between feature variables and target variables
print(diabetes_df.corr()["Diabetes"].sort_values(ascending = False))

# Heatmap showing correlations
plt.figure(figsize=(8, 6))
sns.heatmap(diabetes_df.corr()[['Diabetes']].sort_values(by='Diabetes', ascending=False),
            annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap with Presence of Diabetes')
plt.show()





#Generating Correlation matrix
corr_matrix = diabetes_df.corr()
fig, ax = plt.subplots(figsize=(30, 20))
sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, ax=ax)
ax.set_title('Correlation Matrix')
plt.show()





# Getting descriptive statistics on dataset 
diabetes_df.describe()


# Getting descriptive statistics for population with Diabetes
print(diabetes_df[diabetes_df['Diabetes'] > 0].describe())

# Getting descriptive statistics for population without Diabetes 
print(diabetes_df[diabetes_df['Diabetes'] == 0].describe())


# Generating mean values per feature for both diabetic group and non-diabetic group
print(diabetes_df[diabetes_df['Diabetes'] > 0].mean())
print(diabetes_df[diabetes_df['Diabetes'] == 0].mean())








#Plotting visual for target variable

outcome_counts = diabetes_df['Diabetes'].value_counts()
plt.figure(figsize=(12, 6))
plt.bar(outcome_counts.index, outcome_counts.values, color=['blue', 'red'])
plt.xticks(outcome_counts.index, ['No Diabetes', 'Diabetes'])
plt.xlabel('Diabetes')
plt.ylabel('Count')
plt.title('Distribution of Diabetes Presence')
plt.show()


#Plotting a histogram to check the distribution of Final Fasting Glucose Values

sns.set(style="whitegrid", palette="pastel")
plt.figure(figsize=(10, 5))
sns.histplot(data= diabetes_df['Final Fasting Plasma Glucose'], kde=True, color='skyblue', bins=30)

# Adding title and labels 
plt.title('Distribution of Final Fasting Glucose Values')
plt.xlabel('Final Fasting Glucose Values')
plt.ylabel('Frequency')

plt.show()


#Plotting a histogram to check the distribution of Fasting Plasma Glucose Values

sns.set(style="whitegrid", palette="pastel")
plt.figure(figsize=(10, 5))
sns.histplot(data= diabetes_df['Fasting Plasma Glucose'], kde=True, color='skyblue', bins=30)

# Adding title and labels 
plt.title('Distribution of Fasting Glucose Values')
plt.xlabel('Fasting Glucose Values')
plt.ylabel('Frequency')

plt.show()


#Plotting a histogram to check the distribution of Age values

sns.set(style="whitegrid", palette="pastel")
plt.figure(figsize=(10, 5))
sns.histplot(data= diabetes_df['Age'], kde=True, color='skyblue', bins=30)

# Adding title and labels 
plt.title('Distribution of Age Among Patients')
plt.xlabel('Age')
plt.ylabel('Frequency')

plt.show()


#Histogram of age distribution and presence of diabetes
plt.figure(figsize=(12, 6))
sns.histplot(data=diabetes_df, x='Age', hue='Diabetes', kde=True, bins=20, element='step', common_norm=False)
plt.xlabel('Age', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Histogram of Age by Presence of Diabetes', fontsize=14)
plt.legend(title='Diabetes Status', labels=['Diabetes', 'No Diabetes'])
plt.show()


#Histogram of FFPG distribution and presence of diabetes
plt.figure(figsize=(12, 6))
sns.histplot(data=diabetes_df, x='Final Fasting Plasma Glucose', hue='Diabetes', kde=True, bins=20, element='step', common_norm=False)
plt.xlabel('Final Fasting Plasma Glucose', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Histogram of FFPG Values by Presence of Diabetes', fontsize=14)
plt.legend(title='Diabetes Status', labels=['Diabetes', 'No Diabetes'])
plt.show()


#Histogram of FPG distribution and presence of diabetes
plt.figure(figsize=(12, 6))
sns.histplot(data=diabetes_df, x='Fasting Plasma Glucose', hue='Diabetes', kde=True, bins=20, element='step', common_norm=False)
plt.xlabel('Fasting Plasma Glucose', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Histogram of FPG Values by Presence of Diabetes', fontsize=14)
plt.legend(title='Diabetes Status', labels=['Diabetes', 'No Diabetes'])
plt.show()





# Generating pairplot with selected top 5 highest correlated features

selected_predictors = ['Final Fasting Plasma Glucose', 'Fasting Plasma Glucose', 'Age', 'Systolic Blood Pressure', 'BMI']
sns.set(style="ticks")
sns.pairplot(diabetes_df, hue='Diabetes', vars=selected_predictors, diag_kind='kde', plot_kws={'alpha': 0.7})
plt.suptitle('Pair Plot of Selected Predictors by Presence of Diabetes', fontsize=16, y=1.02)
plt.show()








#Separating target variable from predictors
y = diabetes_df['Diabetes']
X = diabetes_df.drop('Diabetes', axis=1)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Using Smote to address class imbalance with target variable 
sm = SMOTE(random_state = 42)
X_train_resample,y_train_resample = sm.fit_resample(X_train, y_train)

# Scaling the dataset
scaler = StandardScaler()

# Transform the training and test sets
X_train_scaled = scaler.fit_transform(X_train_resample)
X_test_scaled = scaler.transform(X_test)

# Convert into a Dataframe
scaled_data = pd.DataFrame(X_train_scaled, columns = X_train_resample.columns)
scaled_data.head()


#Instantiate the model 
baseline_model = LogisticRegression(random_state = 42)

# Fit the model to the data
baseline_model.fit(X_train_scaled, y_train_resample) 

# Making predictions 
y_base_pred= baseline_model.predict(X_test_scaled)

# Baseline test score 
baseline_score = baseline_model.score(X_test_scaled, y_test)

# Baseline test score 
baseline_train_score = baseline_model.score(X_train_scaled, y_train_resample)

# Cross Validation
baseline_cv = cross_val_score(baseline_model, X_train_scaled, y_train_resample)


print("Baseline Training Score:", baseline_train_score)
print("Baseline Test Score:", baseline_score)
print("Cross Validation Scores:", baseline_cv)


# Classification report for confusion matrix 
baseline_report = classification_report(y_test, y_base_pred)
print(baseline_report)


# Creating function to take in model and data to generate evaluation score metrics
def model_scores(model, X_train, X_test, y_train, y_test):
    
    train_score = model.score(X_train, y_train)
    y_pred_train = model.predict(X_train)
    
    test_score = model.score(X_test, y_test)
    y_pred_test = model.predict(X_test)
    
  
    print("Model's Train Score:", train_score)
    print("Model's Test Score:", test_score)
    print("Cross Validation Scores:", cross_val_score(model, X_train, y_train))
    
    print("Model's Accuracy Score:", accuracy_score(y_test, y_pred_test))
    print("Model's Precision Score:", precision_score(y_test, y_pred_test))
    print("Model's F1 Score:", f1_score(y_test, y_pred_test))
    print("Model's Recall Score:", recall_score(y_test, y_pred_test))


# Generating baseline model scores 
model_scores(baseline_model, X_train_scaled, X_test_scaled, y_train_resample, y_test)











# Decision tree model classifier 
dt_classifier = DecisionTreeClassifier(random_state=42)

# Fit the classifier 
dt_classifier.fit(X_train_scaled, y_train_resample)

# Make predictions for test data
y_tree_pred = dt_classifier.predict(X_test_scaled)

# Scoring on trained data
dt_train_score = dt_classifier.score(X_train_scaled, y_train_resample)

# Scoring on test data
dt_test_score = dt_classifier.score(X_test_scaled, y_test)

# Cross validating model
dt_cv = cross_val_score(dt_classifier, X_train_scaled, y_train_resample)


print("Decision Tree Training Score:", dt_train_score)
print("Decision Tree  Test Score:", dt_test_score)
print("Cross Validation Scores:", dt_cv)


# Classification report 
dt_report = classification_report(y_test, y_tree_pred)
print(dt_report)


# Checking classification metrics
model_scores(dt_classifier, X_train_scaled, X_test_scaled, y_train_resample, y_test)











# Instantiate KNN Model 
knn = KNeighborsClassifier()

knn.fit(X_train_scaled, y_train_resample)
knn_y_pred = knn.predict(X_test_scaled)

# Knn Train Model Score
knn_train_score = knn.score(X_train_scaled, y_train_resample)

# Knn Test Model Score
knn_test_score = knn.score(X_test_scaled, y_test)


# Cross validating model
knn_cv = cross_val_score(knn, X_train_scaled, y_train_resample)


print("KNN Training Score:", knn_train_score)
print("KNN Test Score:", knn_test_score)
print("Cross Validation Scores:", knn_cv)


# Classification report 
knn_report = classification_report(y_test, knn_y_pred)
print(knn_report)


# Checking classification metrics
model_scores(knn, X_train_scaled, X_test_scaled, y_train_resample, y_test)











# Random forest classifier model

rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train_scaled, y_train_resample)

# Getting test predictions
y_rfc_pred = rfc.predict(X_test_scaled)

# Random Forest Train Model Score
rfc_train_score = rfc.score(X_train_scaled, y_train_resample)

# Random Forest Test Model Score
rfc_test_score = rfc.score(X_test_scaled, y_test)

# Cross validating model
rfc_cv = cross_val_score(rfc, X_train_scaled, y_train_resample)


print("Random Forest Training Score:", rfc_train_score)
print("Random Forest Test Score:", rfc_test_score)
print("Cross Validation Scores:", rfc_cv)

# Classification report 
rfc_report = classification_report(y_test, y_rfc_pred)
print(rfc_report)


# Checking classification metrics
model_scores(rfc, X_train_scaled, X_test_scaled, y_train_resample, y_test)











models = [baseline_model, dt_classifier, knn, rfc]
for model in models:
    print(model)
    print("")
    model_scores(model, X_train_scaled, X_test_scaled, y_train_resample, y_test)
    print("")











# Utilizing GridSearch to find best combination of parameters
# Define hyperparameter grid, looking at regularization strength, regularization type and solver options
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  
    'penalty': ['l1', 'l2'],  
    'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],
    'max_iter': [1, 2, 3, 4, 5]
}

# Create a GridSearchCV object
grid_search = GridSearchCV(baseline_model, param_grid, cv=5, scoring='accuracy')

# Fit the grid search to data
grid_search.fit(X_train_scaled, y_train_resample) 


# Print the best hyperparameters and corresponding score
print("Best Hyperparameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)

# Get the best estimator (model with the best hyperparameters)
best_log_reg = grid_search.best_estimator_

# Using tuned model for predictions
y_grid_pred = best_log_reg.predict(X_test_scaled) 



#Instantiate the model with tuned hyperparameters
tuned_baseline_model = LogisticRegression(random_state = 42, C = 1, penalty = 'l2',
                                          solver = 'sag', max_iter = 5 )

# Fit the model to the data
tb_model = tuned_baseline_model.fit(X_train_scaled, y_train_resample) 

# Making predictions 
y_base_tuned_pred= tuned_baseline_model.predict(X_test_scaled)

# Baseline test score 
tuned_baseline_score = tuned_baseline_model.score(X_test_scaled, y_test)

# Baseline test score 
tuned_baseline_train_score = tuned_baseline_model.score(X_train_scaled, y_train_resample)

# Cross Validation
tuned_baseline_cv = cross_val_score(tuned_baseline_model, X_train_scaled, y_train_resample)


print("Tuned Baseline Training Score:", tuned_baseline_train_score)
print("Tuned Baseline Test Score:", tuned_baseline_score)
print("Tuned Cross Validation Scores:", tuned_baseline_cv)



# Classification metrics for tuned baseline model
model_scores(tuned_baseline_model, X_train_scaled, X_test_scaled, y_train_resample, y_test)


# Classification metrics for baseline model
model_scores(baseline_model, X_train_scaled, X_test_scaled, y_train_resample, y_test)








# Utilizing GridSearch to find best combination of parameters for the Random Forest Model

param_grid = {
    'n_estimators': [1, 2, 3, 4, 5],  # Number of decision trees
    'max_depth': [1, 2, 5, 10],  # Maximum depth of each tree
    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],  # Minimum samples required for a leaf node
    'bootstrap': [True, False]  # Whether to use bootstrap samples  
}

# Create a GridSearchCV object
rfc_grid_search = GridSearchCV(rfc, param_grid, cv=5, scoring='accuracy')

# Fit the grid search to data
rfc_grid_search.fit(X_train_scaled, y_train_resample) 


# Print the best hyperparameters and corresponding score
print("Best Hyperparameters: ", rfc_grid_search.best_params_)
print("Best Score: ", rfc_grid_search.best_score_)

# Get the best estimator (model with the best hyperparameters)
best_rfc = rfc_grid_search.best_estimator_

# Using tuned model for predictions
y_rfc_grid_pred = best_rfc.predict(X_test_scaled)  



#Instantiate the model with tuned hyperparameters
tuned_rfc = RandomForestClassifier(random_state = 42, bootstrap = False, max_depth = 10, 
                                   min_samples_leaf = 1, min_samples_split = 2, n_estimators = 5)

# Fit the model to the data
tuned_rfc.fit(X_train_scaled, y_train_resample) 

# Making predictions 
y_rfc_tuned_pred= tuned_rfc.predict(X_test_scaled)

# Tuned test score 
tuned_rfc_test_score = tuned_rfc.score(X_test_scaled, y_test)

# Tuned train score 
tuned_rfc_train_score = tuned_rfc.score(X_train_scaled, y_train_resample)

# Cross Validation
tuned_rfc_cv = cross_val_score(tuned_rfc, X_train_scaled, y_train_resample)


print("Tuned Random Forest Training Score:", tuned_rfc_train_score)
print("Tuned Random Forest Test Score:", tuned_rfc_test_score)
print("Tuned Random Forest Cross Validation Scores:", tuned_rfc_cv)


# Classification metrics for best rfc model
model_scores(tuned_rfc, X_train_scaled, X_test_scaled, y_train_resample, y_test)


# Checking classification for initial rfc metrics
model_scores(rfc, X_train_scaled, X_test_scaled, y_train_resample, y_test)








# Getting the coefficients (weights) of the features
coef = tuned_baseline_model.coef_[0]

# Calculating the feature importances by taking the absolute values of the coefficients
feature_importances = abs(coef)

# Getting the feature names 
feature_names = ["Age", "Gender", "BMI", "Systolic Blood Pressure", "Diastolic Blood Pressure", "Fasting Plasma Glucose", 
                 "Cholesterol", "Triglycerides", "HDL", "LDL", "ALT", "BUN", "Creatine Clearance", "Final Fasting Plasma Glucose", 
                 "Smoking", "Drinking", "Family_History"]

# Creating a list of (feature_name, importance) pairs
feature_importance_pairs = list(zip(feature_names, feature_importances))

# Sorting the feature importances by importance in descending order
feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)

# Printting feature importances
for feature, importance in feature_importance_pairs:
    print(f"{feature}: {importance}")

# Plotting feature importances 
plt.figure(figsize=(10, 6))
plt.barh([feature for feature, importance in feature_importance_pairs], 
         [importance for feature, importance in feature_importance_pairs])
plt.xlabel('Feature Importance')
plt.title('Feature Importances for Tuned Baseline Model')
plt.gca().invert_yaxis() 
plt.show()

    








# Print confusion matrix
cnf_matrix = confusion_matrix(y_test, y_base_tuned_pred)
print('Confusion Matrix:\n', cnf_matrix)


# Visualize confusion matrix
tbcf_disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix, display_labels=tb_model.classes_)
tbcf_disp.plot(cmap=plt.cm.Blues)
























